{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x110ee0150> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x110ed6810> model_name='openai/gpt-oss-120b' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Generative AI** refers to a class of artificial‑intelligence systems that can create new content—such as text, images, audio, video, code, or even 3‑D models—rather than just analyzing or classifying existing data.  \\n\\n### How It Works\\n1. **Training on Large Datasets**  \\n   - The model learns patterns, structures, and relationships from massive amounts of example data (e.g., billions of words, millions of photos).\\n\\n2. **Probabilistic Generation**  \\n   - When given a prompt or a conditioning signal, the model samples from the probability distribution it learned, producing output that resembles the training data but is novel.\\n\\n3. **Feedback & Fine‑Tuning**  \\n   - Techniques like reinforcement learning from human feedback (RLHF) or supervised fine‑tuning help align the model’s outputs with desired qualities (relevance, safety, style).\\n\\n### Common Types of Generative Models\\n| Model Family | Core Architecture | Typical Output | Example Systems |\\n|--------------|-------------------|----------------|-----------------|\\n| **Transformer‑based language models** | Decoder‑only (e.g., GPT) or encoder‑decoder (e.g., T5) | Text, code, structured data | ChatGPT, Claude, LLaMA |\\n| **Diffusion models** | Iterative denoising process | Images, video, audio | Stable Diffusion, DALL‑E 3, Imagen |\\n| **GANs (Generative Adversarial Networks)** | Generator vs. discriminator competition | Images, video, 3‑D objects | StyleGAN, BigGAN |\\n| **Variational Autoencoders (VAEs)** | Probabilistic encoding/decoding | Images, speech, latent‑space manipulation | VQ‑VAE‑2 |\\n| **Neural Radiance Fields (NeRFs)** | Implicit 3‑D scene representation | 3‑D view synthesis | DreamFusion, Instant‑NGP |\\n\\n### Real‑World Applications\\n- **Content creation:** Blog posts, marketing copy, scripts, music, artwork, product designs.  \\n- **Design assistance:** Rapid prototyping, UI mockups, architectural visualizations.  \\n- **Software development:** Code autocompletion, bug‑fix suggestions, documentation generation.  \\n- **Data augmentation:** Synthetic training data for computer‑vision or NLP models.  \\n- **Personalized experiences:** Conversational agents, tutoring systems, virtual companions.  \\n- **Scientific discovery:** Molecule generation, protein folding predictions, hypothesis generation.\\n\\n### Benefits\\n- **Speed & scale:** Produce drafts or prototypes in seconds that would take humans hours or days.  \\n- **Creativity boost:** Offer novel ideas or variations that a human might not think of.  \\n- **Accessibility:** Lower the barrier for non‑experts to generate high‑quality media or code.\\n\\n### Risks & Challenges\\n- **Misinformation & deepfakes:** Realistic synthetic content can be misused.  \\n- **Bias & fairness:** Models inherit biases present in their training data.  \\n- **Intellectual‑property concerns:** Generated works may unintentionally replicate copyrighted material.  \\n- **Safety & alignment:** Ensuring outputs are appropriate, factual, and non‑harmful requires robust guardrails.  \\n\\n### Key Takeaway\\nGenerative AI is about **machines that *create***—leveraging learned statistical patterns to produce new, useful, and often surprising content across many domains. Its rapid advances are reshaping how we produce media, code, and ideas, while also prompting important discussions about ethics, regulation, and responsible use.' additional_kwargs={'reasoning_content': 'The user asks: \"What is generative AI?\" Provide a concise explanation. Probably simple.'} response_metadata={'token_usage': {'completion_tokens': 762, 'prompt_tokens': 77, 'total_tokens': 839, 'completion_time': 1.6349333449999999, 'prompt_time': 0.002875411, 'queue_time': 0.053779808, 'total_time': 1.637808756, 'completion_tokens_details': {'reasoning_tokens': 20}}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_fd1fe7f861', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--80d8c4e5-b652-4ee1-8f17-bdb5a814b0a6-0' usage_metadata={'input_tokens': 77, 'output_tokens': 762, 'total_tokens': 839}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**LangSmith** (often styled as **LangSmith**) is the observability, testing, and debugging platform that LangChain built to help developers and teams ship reliable LLM‑powered applications faster. Think of it as the “DevOps” layer for generative‑AI pipelines.\\n\\n---\\n\\n## 1. What LangSmith Does\\n\\n| Area | What It Provides | Why It Matters |\\n|------|------------------|----------------|\\n| **Tracing & Observability** | Automatic capture of every LLM call, tool invocation, and chain execution (inputs, outputs, timestamps, token usage, latency, etc.). | Gives you a full, searchable audit trail—critical for debugging, compliance, and performance tuning. |\\n| **Testing & CI/CD** | Unit‑style tests for prompts, chains, agents, and custom tools. Tests can be run locally, in CI pipelines, or on a schedule. | Prevents regressions when you change prompts, upgrade models, or refactor code. |\\n| **Evaluation & Metrics** | Built‑in metrics (e.g., accuracy, relevance, hallucination score) plus custom metric hooks. Supports A/B testing across model versions or prompt variants. | Quantifies quality, helps you pick the best model/prompts, and justifies cost decisions. |\\n| **Prompt Management** | Versioned prompt store, diff view, rollout controls, and the ability to tag prompts with metadata (e.g., “production”, “experimental”). | Keeps prompt evolution transparent and reproducible. |\\n| **Cost & Usage Monitoring** | Real‑time token and dollar spend dashboards per project, per model, per endpoint. | Stops surprise bills and enables budget‑aware model selection. |\\n| **Collaboration** | Shared workspaces, role‑based permissions, comment threads on traces, and the ability to “pin” a trace as a bug report. | Teams can troubleshoot together and hand off work without losing context. |\\n| **Integrations** | Native SDK for LangChain (Python, TypeScript/JS), REST API, and first‑class support for LangChain‑Hub components, OpenAI, Anthropic, Cohere, Azure, AWS Bedrock, etc. | You can instrument any LangChain‑based app with a single line of code. |\\n| **Deploy‑able Options** | SaaS (hosted by LangChain) and an on‑prem/self‑hosted Docker image for enterprises with strict data‑sovereignty requirements. | Flexibility for both startups and regulated industries. |\\n\\n---\\n\\n## 2. How It Works (High‑Level Flow)\\n\\n1. **Instrumentation** – Add `langsmith` to your LangChain project (often just `import langsmith` and `langsmith.init()` or set `LANGCHAIN_TRACING_V2=true`).  \\n2. **Run Your App** – Every `LLM`, `Chain`, `Agent`, or custom `Tool` automatically logs a **trace** to LangSmith.  \\n3. **Store & Index** – LangSmith stores traces in a searchable time‑series database, linking parent/child calls (e.g., a chain → LLM → tool).  \\n4. **Dashboard** – You view traces, filter by tags, model, latency, cost, etc., and drill into any step to see raw inputs/outputs.  \\n5. **Testing** – Write tests like `assert run.output == \"expected\"`; LangSmith runs them, records pass/fail, and surfaces failures in the UI or CI.  \\n6. **Evaluation** – Attach evaluation functions (e.g., semantic similarity, factuality check) that score each trace; aggregate scores appear as charts.  \\n7. **Alerting** – Set thresholds (e.g., latency > 2\\u202fs, cost > $100/day) and receive Slack/email/webhook alerts.  \\n\\n---\\n\\n## 3. Core Concepts & Terminology\\n\\n| Term | Meaning |\\n|------|---------|\\n| **Trace** | A single execution of an LLM‑driven workflow (chain, agent, etc.). Contains a tree of **runs** (each LLM call, tool call, or sub‑chain). |\\n| **Run** | One node in the trace tree. Has metadata: `run_id`, `parent_run_id`, `name`, `inputs`, `outputs`, `start_time`, `end_time`, `tags`, `metadata`. |\\n| **Project** | Logical grouping of traces (e.g., “customer‑support‑bot”, “research‑assistant”). Each project gets its own dashboard & quota. |\\n| **Dataset** | Collection of input‑output pairs you can use for evaluation (e.g., a list of questions + gold answers). |\\n| **Evaluation** | A function that consumes a trace (or a dataset) and returns a numeric score or pass/fail flag. |\\n| **Prompt Version** | A stored prompt template with a unique version ID; you can roll back or compare diffs. |\\n| **API Key** | Auth token used by SDK/CLI to send data to LangSmith. Managed per user/team. |\\n\\n---\\n\\n## 4. Getting Started (Python Example)\\n\\n```python\\n# 1️⃣ Install\\npip install langchain langsmith\\n\\n# 2️⃣ Initialise (once, usually at app start)\\nimport os\\nfrom langsmith import Client, traceable\\n\\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"          # turn on tracing\\nos.environ[\"LANGCHAIN_API_KEY\"] = \"ls_XXXXXXXXXXXXXXXX\"  # your LangSmith API key\\nos.environ[\"LANGCHAIN_PROJECT\"] = \"my-qa-bot\"       # optional, defaults to \"default\"\\n\\n# 3️⃣ Build a simple chain\\nfrom langchain.llms import OpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.chains import LLMChain\\n\\nprompt = PromptTemplate(\\n    input_variables=[\"question\"],\\n    template=\"You are a helpful assistant. Answer briefly:\\\\n\\\\n{question}\"\\n)\\n\\nllm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\\nqa_chain = LLMChain(prompt=prompt, llm=llm)\\n\\n# 4️⃣ Run it – a trace is automatically created\\nanswer = qa_chain.run({\"question\": \"What is the capital of France?\"})\\nprint(answer)   # => \"Paris\"\\n```\\n\\nAll the above steps generate a trace you can explore at `https://smith.langchain.com`.\\n\\n### Testing Example\\n\\n```python\\nfrom langsmith import evaluate\\nfrom langchain.evaluation import StringEvaluator\\n\\nevaluator = StringEvaluator()   # simple exact‑match evaluator\\n\\n@evaluate\\ndef test_qa_chain():\\n    output = qa_chain.run({\"question\": \"What is 2+2?\"})\\n    return evaluator.evaluate_strings(prediction=output, reference=\"4\")\\n```\\n\\nRunning `pytest` (or any CI runner) will push the test result to LangSmith, where you can see pass/fail trends over time.\\n\\n---\\n\\n## 5. Pricing (as of 2024‑09)\\n\\n| Tier | Monthly Cost | Included Traces | Features |\\n|------|--------------|----------------|----------|\\n| **Free** | $0 | 10\\u202fk trace events, 1\\u202fM tokens, 1 project | Basic tracing, dashboards, limited retention (7\\u202fdays). |\\n| **Team** | $49 / user | 500\\u202fk trace events, 10\\u202fM tokens, 5 projects | Full dashboards, alerts, test scheduling, role‑based access. |\\n| **Enterprise** | Custom | Unlimited (or negotiated caps) | Dedicated VPC/self‑hosted option, SSO/SAML, audit logs, SLA, priority support. |\\n\\n> **Note:** Prices and quotas evolve; always check the latest pricing page on the LangSmith site.\\n\\n---\\n\\n## 6. Typical Use‑Cases\\n\\n| Use‑Case | How LangSmith Helps |\\n|----------|---------------------|\\n| **Production LLM Service** | Real‑time latency & cost dashboards; alerts when a model degrades or costs spike. |\\n| **Prompt Engineering** | Versioned prompts + A/B evaluation graphs to see which phrasing yields higher accuracy. |\\n| **Regulated Industries** (finance, health) | Full audit trail of every LLM call, immutable logs for compliance auditors. |\\n| **Team Collaboration** | Shared workspaces, comments on failing traces, hand‑off from data‑science to engineering. |\\n| **CI/CD for LLM Apps** | Automated tests that run on every PR; failing tests block merges, preventing regressions. |\\n| **Model Migration** | Compare token usage, latency, and quality side‑by‑side when moving from `gpt‑3.5‑turbo` to `gpt‑4o`. |\\n| **Cost Optimization** | Spot high‑token calls, replace with cheaper embeddings or cached results. |\\n\\n---\\n\\n## 7. Security & Data Governance\\n\\n| Feature | Detail |\\n|---------|--------|\\n| **Transport** | All traffic encrypted via TLS 1.3. |\\n| **Data at Rest** | AES‑256 encryption for stored traces. |\\n| **Retention Controls** | Configurable per‑project retention (7\\u202fdays → 90\\u202fdays → indefinite). |\\n| **PII Redaction** | SDK can automatically mask fields (e.g., `user_email`, `ssn`) before sending. |\\n| **Self‑Hosted** | Docker image (`langsmith/selfhosted`) that runs behind your firewall; you control the database. |\\n| **Compliance** | SOC\\u202f2 Type\\u202fII, GDPR‑compliant, and ISO\\u202f27001 certifications (for the SaaS offering). |\\n\\n---\\n\\n## 8. Ecosystem & Integrations\\n\\n- **LangChain SDKs** – Python, TypeScript/JS, Java (beta).  \\n- **LangChain Hub** – Prompt & chain templates can be published with LangSmith metadata.  \\n- **Observability Stack** – Export traces to Datadog, New Relic, or OpenTelemetry via webhook.  \\n- **CI Tools** – GitHub Actions, GitLab CI, CircleCI have ready‑made steps (`langsmith test`).  \\n- **Alert Channels** – Slack, Microsoft Teams, PagerDuty, email, custom webhook.  \\n\\n---\\n\\n## 9. Limitations & Gotchas\\n\\n| Issue | Mitigation |\\n|-------|------------|\\n| **Large Payloads** | Traces store raw inputs/outputs; for huge documents (>10\\u202fKB) you may want to store a hash or a reference to external storage (S3, DB). |\\n| **Cost of Tracing** | Enabling tracing for every request in a high‑throughput production system can quickly consume free quotas. Use sampling (`LANGCHAIN_TRACING_SAMPLE_RATE`) to limit. |\\n| **Version Drift** | If you change the SDK version without migrating old traces, schema mismatches can appear. Stick to a single SDK version per project or run a migration script. |\\n| **Self‑Hosted Scaling** | The on‑prem image uses PostgreSQL + ClickHouse; you must provision enough storage and compute for high‑volume workloads. |\\n| **Prompt Diffing** | LangSmith shows diffs at the text level but does not automatically surface semantic changes; combine with evaluation metrics for a fuller picture. |\\n\\n---\\n\\n## 10. Quick Reference Cheat‑Sheet\\n\\n| Command | What It Does |\\n|---------|--------------|\\n| `export LANGCHAIN_TRACING_V2=true` | Turn on automatic tracing (default in recent LangChain releases). |\\n| `export LANGCHAIN_API_KEY=ls_…` | Set your API key. |\\n| `langsmith dashboard` | Opens the web UI in your default browser. |\\n| `langsmith dataset create --name qa-test` | Create a dataset for evaluation. |\\n| `langsmith evaluate run --run-id <id> --evaluator my_evaluator.py` | Run a custom evaluator on a single trace. |\\n| `langsmith project list` | List all projects you have access to. |\\n| `langsmith quota` | Show remaining trace and token quota for the current month. |\\n| `langsmith selfhost start` | Spin up the Docker‑compose stack for on‑prem deployment. |\\n\\n---\\n\\n## 11. Where to Learn More\\n\\n- **Official Docs:** https://docs.smith.langchain.com  \\n- **Quick‑Start Notebook:** https://github.com/langchain-ai/langsmith-quickstart (Python & TypeScript).  \\n- **Blog Post (2024‑08):** “Observability for LLM Apps – Introducing LangSmith” – deep dive into architecture.  \\n- **Community Slack:** `#langsmith` channel in the LangChain community workspace.  \\n- **Video Walkthrough:** LangChain YouTube playlist “LangSmith in 15 minutes”.\\n\\n---\\n\\n### TL;DR\\n\\nLangSmith is a **full‑stack observability, testing, and evaluation platform** for LangChain‑based LLM applications. It automatically logs every LLM call, lets you version prompts, write unit‑style tests, monitor cost/latency, and collaborate across teams. With a free tier for hobby projects and enterprise‑grade self‑hosted options, it’s become the go‑to “DevOps” layer for anyone building production LLM services. If you’re already using LangChain, adding a single line of config gives you instant visibility and safety nets for your AI product.' additional_kwargs={'reasoning_content': 'The user asks: \"Can you tell me about Langsmith?\" Likely they refer to LangSmith, a product by LangChain (LangChain\\'s platform for LLM development). Provide overview: what is LangSmith, its features, capabilities, pricing, use cases, integration with LangChain, how to get started, etc. Should be concise but thorough. No disallowed content. Provide helpful answer.'} response_metadata={'token_usage': {'completion_tokens': 2848, 'prompt_tokens': 96, 'total_tokens': 2944, 'completion_time': 6.030299784, 'prompt_time': 0.003597547, 'queue_time': 0.054403293, 'total_time': 6.033897331, 'completion_tokens_details': {'reasoning_tokens': 82}}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_fd1fe7f861', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--ba21f965-d776-40be-93f3-1da850e405c9-0' usage_metadata={'input_tokens': 96, 'output_tokens': 2848, 'total_tokens': 2944}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a LangChain platform for tracking, debugging, and managing LLM prompts, pipelines, and performance metrics across development teams efficiently.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith in 20 words?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentnotes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

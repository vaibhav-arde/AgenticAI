{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "# ## Langsmith Tracking\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "# llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "# print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaibhavarde/Desktop/AgentKrish/AgentNotes/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True} client=<groq.resources.chat.completions.Completions object at 0x118d46010> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x118ea5a90> model_name='openai/gpt-oss-120b' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"openai/gpt-oss-120b\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Generative AI** is a class of artificial‑intelligence systems that can create new content—such as text, images, audio, video, code, or even 3‑D models—rather than just analyzing or classifying existing data.  \\n\\n---\\n\\n## Core Idea\\n- **Learning from data:** A generative model is trained on large datasets of examples (e.g., millions of sentences, photos, or code snippets).  \\n- **Capturing patterns:** During training it learns the statistical relationships and structures that underlie the data.  \\n- **Sampling new outputs:** After training, the model can *sample* from the learned distribution to produce novel items that resemble the training data but aren’t direct copies.\\n\\n---\\n\\n## Main Types of Generative Models\\n\\n| Model Family | Typical Architecture | What It Generates | Key Strengths |\\n|--------------|----------------------|-------------------|---------------|\\n| **Autoregressive models** | Transformers (e.g., GPT‑4, LLaMA) | Text, code, music, sometimes images (when combined with diffusion) | High-quality, coherent sequential output; strong language understanding |\\n| **Variational Autoencoders (VAEs)** | Encoder‑decoder neural nets with a latent space | Images, audio, 3‑D shapes | Explicit latent representation; smooth interpolation between concepts |\\n| **Generative Adversarial Networks (GANs)** | Two networks (generator vs. discriminator) | Photorealistic images, video frames, style transfer | Very sharp, realistic visuals; good for high‑resolution synthesis |\\n| **Diffusion models** | Gradual denoising process (e.g., Stable Diffusion, DALL‑E 2) | Images, video, audio | Excellent image quality, controllable generation, scalable to large datasets |\\n| **Flow‑based models** | Invertible transformations (e.g., RealNVP, Glow) | Images, audio | Exact likelihood computation; reversible generation |\\n| **Hybrid / multimodal models** | Combine the above (e.g., CLIP‑guided diffusion, Flamingo) | Text‑to‑image, image‑to‑text, video‑with‑audio, etc. | Seamless cross‑modal generation |\\n\\n---\\n\\n## How It Works (Simplified)\\n\\n1. **Data Collection** – Gather a massive, diverse corpus of the target modality (e.g., web text, stock photos).  \\n2. **Pre‑processing** – Tokenize text, normalize images, convert audio to spectrograms, etc.  \\n3. **Model Training** – Optimize a loss function that encourages the model to predict the next token/pixel/latent variable given the context (e.g., next‑word prediction for GPT, denoising objective for diffusion).  \\n4. **Sampling / Inference** – Provide a prompt or conditioning signal (e.g., “a cat riding a skateboard”) and let the model iteratively generate the output, often using strategies like beam search, top‑k/top‑p sampling, or diffusion steps.  \\n5. **Post‑processing** – Clean up artifacts, upscale images, filter unsafe content, or convert tokens back into human‑readable form.\\n\\n---\\n\\n## Real‑World Applications\\n\\n| Domain | Example Uses |\\n|--------|--------------|\\n| **Content creation** | Blog posts, marketing copy, scripts, poetry, music composition |\\n| **Design & Art** | AI‑generated illustrations, concept art, UI mockups, 3‑D assets |\\n| **Software development** | Code completion (GitHub Copilot), automated test generation, documentation |\\n| **Healthcare** | Synthetic medical images for training, drug‑molecule generation, clinical note drafting |\\n| **Gaming & Entertainment** | Procedurally generated levels, NPC dialogue, deep‑fake avatars |\\n| **Education** | Personalized tutoring, question‑generation, language practice |\\n| **Business intelligence** | Summarizing reports, generating data visualizations, scenario simulation |\\n| **Scientific research** | Simulating physical systems, generating hypotheses, drafting papers |\\n\\n---\\n\\n## Benefits\\n\\n- **Speed & scale:** Produce large volumes of content in seconds.  \\n- **Creativity boost:** Offer novel ideas or variations that humans might not think of.  \\n- **Personalization:** Tailor output to individual preferences or contexts.  \\n- **Cost reduction:** Lower the need for manual labor in repetitive generation tasks.\\n\\n---\\n\\n## Challenges & Risks\\n\\n| Issue | Why It Matters | Mitigations |\\n|-------|----------------|-------------|\\n| **Hallucinations / factual errors** | Model may fabricate plausible‑looking but false information. | Fact‑checking pipelines, retrieval‑augmented generation, human review. |\\n| **Bias & fairness** | Training data can embed societal biases, leading to discriminatory outputs. | Diverse datasets, bias‑aware fine‑tuning, output filters. |\\n| **Intellectual‑property concerns** | Generated content can inadvertently copy copyrighted material. | Use licensed data, watermarking, legal compliance checks. |\\n| **Misuse (deepfakes, disinformation)** | Realistic media can be weaponized. | Watermarking, detection tools, policy & access controls. |\\n| **Resource intensity** | Large models need massive compute and energy. | Model distillation, efficient architectures, hardware optimization. |\\n| **Safety & alignment** | Ensuring the model behaves according to user intent and ethical norms. | Reinforcement learning from human feedback (RLHF), guardrails, continuous monitoring. |\\n\\n---\\n\\n## Quick‑Start Checklist if You Want to Build or Use Generative AI\\n\\n1. **Define the output modality & quality goals** (e.g., “high‑resolution photorealistic images”).  \\n2. **Select an appropriate model family** (diffusion for images, transformer for text).  \\n3. **Gather and curate a representative dataset**—ensure licensing and diversity.  \\n4. **Choose a training strategy** (full training vs. fine‑tuning a pre‑trained model).  \\n5. **Implement safety layers** (prompt filtering, toxicity classifiers, plagiarism checks).  \\n6. **Set up evaluation metrics** (BLEU/ROUGE for text, FID/IS for images, human preference studies).  \\n7. **Deploy with scalable inference** (GPU/TPU serving, quantization, caching).  \\n8. **Monitor post‑deployment** for drift, misuse, and performance degradation.\\n\\n---\\n\\n### TL;DR\\nGenerative AI refers to models that *learn* from large datasets and then *produce* new, original content—whether it’s words, pictures, music, code, or beyond. It leverages architectures like transformers, diffusion models, GANs, and VAEs, enabling applications from automated writing to AI‑crafted art. While the technology unlocks massive creative and productivity gains, it also brings challenges around accuracy, bias, copyright, and misuse that must be addressed through careful design, monitoring, and responsible governance.' additional_kwargs={'reasoning_content': 'The user asks: \"What is generative AI?\" Provide explanation. Should be concise but thorough. No policy issues. Provide definition, examples, how it works, applications, challenges.'} response_metadata={'token_usage': {'completion_tokens': 1459, 'prompt_tokens': 77, 'total_tokens': 1536, 'completion_time': 3.130641472, 'completion_tokens_details': {'reasoning_tokens': 38}, 'prompt_time': 0.00287639, 'prompt_tokens_details': None, 'queue_time': 0.054158739, 'total_time': 3.133517862}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a28df4bce5', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--f1cef2ec-cfdd-4ac2-b558-2bec86d5bfb4-0' usage_metadata={'input_tokens': 77, 'output_tokens': 1459, 'total_tokens': 1536, 'output_token_details': {'reasoning': 38}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='### LangSmith – Overview\\n\\n**LangSmith** is the observability, debugging, testing, and evaluation platform built by the creators of **LangChain**. It gives developers and data scientists a centralized place to monitor, trace, and improve the performance of LLM‑driven applications (agents, chatbots, RAG pipelines, etc.). Think of it as “Datadog for LLM apps” – it captures every request, logs prompts and responses, tracks token usage, and lets you run systematic evaluations.\\n\\n| Category | What LangSmith Provides |\\n|----------|------------------------|\\n| **Observability** | Real‑time dashboards, request tracing, latency & token‑usage metrics, error alerts |\\n| **Debugging** | Prompt replay, step‑by‑step view of chain execution, variable inspection, versioned runs |\\n| **Evaluation** | Built‑in test suite, custom metrics (e.g., factuality, relevance, toxicity), automated regression testing |\\n| **Collaboration** | Shared workspaces, run tagging, comment threads, exportable reports |\\n| **Integrations** | Native LangChain hooks, OpenAI, Anthropic, Cohere, Azure OpenAI, Hugging Face, custom LLM wrappers |\\n| **Security & Governance** | Role‑based access control, data redaction, audit logs, optional on‑prem deployment (enterprise) |\\n\\n---\\n\\n## Core Concepts\\n\\n| Concept | Description |\\n|---------|-------------|\\n| **Run** | One execution of a LangChain (or any LLM‑powered) workflow. Every run is stored with its input, output, intermediate steps, and metadata. |\\n| **Trace** | A hierarchical view of a run, showing each node (prompt, tool call, chain, agent step) with timestamps and token counts. |\\n| **Dataset** | A collection of test examples (inputs + expected outputs) you can use to evaluate a model or chain. |\\n| **Evaluation** | A function (or LangChain `Evaluator`) that scores a run against a dataset. Results are stored and visualized. |\\n| **Feedback Loop** | You can label runs (e.g., “correct”, “incorrect”) and feed that back into model fine‑tuning or prompt engineering. |\\n| **Workspace** | Logical grouping of runs, datasets, and evaluations (e.g., per project, team, or product). |\\n\\n---\\n\\n## Typical Workflow\\n\\n1. **Instrument Your Code**  \\n   ```python\\n   from langsmith import Client, traceable\\n\\n   client = Client()                     # reads LANGCHAIN_API_KEY env var\\n   @traceable\\n   def my_chain(...):\\n       # your LangChain logic\\n   ```\\n   The decorator automatically logs each run to LangSmith.\\n\\n2. **Run the Application**  \\n   Every request (API call, UI interaction, batch job) creates a run in the platform.\\n\\n3. **Explore Traces**  \\n   - Click a run → see the full prompt‑response tree.  \\n   - Inspect variable values at each step.  \\n   - Spot where a chain failed or produced hallucinations.\\n\\n4. **Create a Test Dataset**  \\n   ```python\\n   from langsmith import Dataset\\n\\n   ds = Dataset(name=\"qa_faq\")\\n   ds.add_examples([\\n       {\"input\": \"What is the refund policy?\", \"expected_output\": \"…\"},\\n       # …\\n   ])\\n   ```\\n\\n5. **Run Evaluations**  \\n   ```python\\n   from langchain.evaluation import QAEvalChain\\n   evaluator = QAEvalChain()\\n   results = ds.evaluate(evaluator)\\n   ```\\n\\n6. **Analyze Metrics**  \\n   Dashboard shows accuracy, latency, token cost, and custom scores over time. You can filter by model version, prompt version, or tag.\\n\\n7. **Iterate**  \\n   - Update prompts, chain logic, or model.  \\n   - Re‑run evaluations; LangSmith highlights regressions automatically.  \\n\\n8. **Collaboration & Reporting**  \\n   Share a workspace link, export CSV/JSON, embed charts in Confluence, or set up Slack alerts for anomalies.\\n\\n---\\n\\n## Key Features in Detail\\n\\n### 1. **Full‑stack Tracing**\\n- **Granular steps**: each `PromptTemplate`, `LLM`, `Tool`, `Retriever`, and `AgentAction` appears as a node.\\n- **Token accounting**: counts per node, cost estimation (e.g., $ per 1k tokens).\\n- **Latency breakdown**: network latency vs. LLM inference time.\\n\\n### 2. **Prompt & Chain Versioning**\\n- Every run stores the exact prompt text and chain configuration.\\n- Diff view lets you compare two runs and see what changed (useful for A/B testing).\\n\\n### 3. **Automated Evaluation Suite**\\n- **Built‑in evaluators**: `qa`, `summarization`, `translation`, `toxicity`, `faithfulness`.\\n- **Custom evaluators**: Write a Python function that receives `(input, output, expected)` and returns a score/metadata.\\n- **Batch evaluation**: Run on thousands of examples with parallel workers.\\n\\n### 4. **Alerting & Monitoring**\\n- Define thresholds (e.g., “accuracy < 85% for 3 consecutive runs”) → Slack, email, PagerDuty.\\n- Anomaly detection on latency or cost spikes.\\n\\n### 5. **Security & Compliance**\\n- **Redaction rules**: mask PII or proprietary text before it hits the cloud.\\n- **SOC‑2 / ISO‑27001** compliance for the hosted service.\\n- **Enterprise on‑prem** option (Docker/Kubernetes) for highly regulated environments.\\n\\n### 6. **Collaboration Tools**\\n- **Run tagging** (e.g., `prod`, `staging`, `experiment-xyz`).\\n- **Comment threads** attached to runs for peer review.\\n- **Exportable reports** (PDF/HTML) for stakeholder updates.\\n\\n---\\n\\n## Pricing (as of early\\u202f2025)\\n\\n| Tier | Monthly Cost | Core Limits | Notable Inclusions |\\n|------|--------------|-------------|--------------------|\\n| **Free** | $0 | 5,000 runs, 10\\u202fk evaluation calls, 1 workspace | Basic tracing, limited retention (30\\u202fdays) |\\n| **Pro** | $49 / month | 100\\u202fk runs, 200\\u202fk eval calls, 5 workspaces | Unlimited retention, alerts, custom evaluators |\\n| **Team** | $199 / month | 500\\u202fk runs, 1\\u202fM eval calls, 20 workspaces | Role‑based access, SSO, audit logs |\\n| **Enterprise** | Custom | Unlimited | Dedicated VPC, on‑prem deployment, SLA, priority support, compliance consulting |\\n\\n*All paid plans include a free trial of the next tier for 14\\u202fdays.*\\n\\n---\\n\\n## How to Get Started\\n\\n1. **Create an account** at https://langsmith.com (Google/GitHub SSO works).\\n2. **Generate an API key** (found under *Settings → API*).  \\n   Set it as an environment variable:\\n   ```bash\\n   export LANGCHAIN_API_KEY=sk-xxxx\\n   ```\\n3. **Install the SDK**:\\n   ```bash\\n   pip install langsmith langchain\\n   ```\\n4. **Add the decorator** (or use the built‑in LangChain integration):\\n   ```python\\n   from langchain import LLMChain, PromptTemplate\\n   from langsmith import traceable\\n\\n   @traceable\\n   def my_qa_chain(question: str):\\n       prompt = PromptTemplate.from_template(\"Answer the question: {question}\")\\n       chain = LLMChain(llm=OpenAI(), prompt=prompt)\\n       return chain.run(question=question)\\n   ```\\n5. **Visit the dashboard** → you’ll see runs appear instantly.\\n\\n---\\n\\n## Common Use‑Cases\\n\\n| Use‑Case | How LangSmith Helps |\\n|----------|---------------------|\\n| **Production chatbot** | Detect sudden drops in response relevance, monitor cost per conversation, roll back to previous prompt version automatically. |\\n| **RAG pipeline** | Trace which document retriever returned which chunk, compare different retriever configs, evaluate factuality of generated answers. |\\n| **Agent orchestration** | Visualize tool‑call sequences, spot infinite loops, measure latency per tool. |\\n| **Model fine‑tuning** | Tag runs with “good” vs. “bad” outputs, export labeled data for supervised fine‑tuning. |\\n| **Compliance audit** | Export full request/response logs with redaction for regulators. |\\n\\n---\\n\\n## Alternatives & Complementary Tools\\n\\n| Tool | Focus | Notable Differences |\\n|------|-------|----------------------|\\n| **PromptLayer** | Prompt versioning & logging | More lightweight, less built‑in evaluation; integrates with many LLM providers. |\\n| **Weights & Biases (W&B) – LLM tracking** | General ML experiment tracking | Strong in hyper‑parameter sweeps, but requires more manual instrumentation for LLM chains. |\\n| **Arize AI** | Model monitoring (including LLMs) | Enterprise‑grade drift detection, but UI is less chain‑oriented. |\\n| **OpenAI Playground + Logs** | Quick debugging | No persistent run history or collaborative features. |\\n| **LangChain Hub** | Prompt/chain sharing | Not an observability platform; can be combined with LangSmith for full stack. |\\n\\n---\\n\\n## Tips & Best Practices\\n\\n1. **Tag early & often** – use tags like `model:gpt-4o`, `env:prod`, `release:2025-09`. They make filtering painless.\\n2. **Redact sensitive data** – define a regex or custom hook to mask PII before logging.\\n3. **Set cost alerts** – token usage can balloon; an alert at, say, $500/month prevents surprise bills.\\n4. **Automate regression tests** – integrate `langsmith evaluate` into your CI pipeline (`pytest` + GitHub Actions).\\n5. **Leverage “Run Snapshots”** – snapshot a run before a major change; you can replay it later to compare side‑by‑side.\\n6. **Use “Eval Suites” for multi‑metric checks** – combine factuality, toxicity, and latency into a single dashboard view.\\n\\n---\\n\\n## Quick Sample Code (End‑to‑End)\\n\\n```python\\n# 1️⃣ Install\\n# pip install langsmith langchain openai\\n\\n# 2️⃣ Set env vars\\nimport os\\nos.environ[\"LANGCHAIN_API_KEY\"] = \"sk-xxxx\"\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-openai-xxxx\"\\n\\n# 3️⃣ Instrument a simple QA chain\\nfrom langchain import OpenAI, PromptTemplate, LLMChain\\nfrom langsmith import traceable, Client, Dataset\\n\\n@traceable   # <-- auto‑logs every call to LangSmith\\ndef answer_question(question: str) -> str:\\n    prompt = PromptTemplate.from_template(\\n        \"You are a helpful assistant. Answer the question concisely.\\\\nQ: {question}\\\\nA:\"\\n    )\\n    chain = LLMChain(llm=OpenAI(model=\"gpt-4o-mini\"), prompt=prompt)\\n    return chain.run(question=question)\\n\\n# 4️⃣ Run a few examples\\nfor q in [\\n    \"What is the capital of Canada?\",\\n    \"Explain quantum entanglement in two sentences.\"\\n]:\\n    print(answer_question(q))\\n\\n# 5️⃣ Create a test dataset & evaluate\\nds = Dataset(name=\"qa_demo\")\\nds.add_examples([\\n    {\"input\": \"What is the capital of Canada?\", \"expected_output\": \"Ottawa\"},\\n    {\"input\": \"Who wrote \\'Pride and Prejudice\\'?\", \"expected_output\": \"Jane Austen\"},\\n])\\n\\nfrom langchain.evaluation import QAEvalChain\\nevaluator = QAEvalChain()\\nresults = ds.evaluate(evaluator)   # pushes scores to LangSmith\\n\\nprint(\"Evaluation completed. Check the LangSmith dashboard for details.\")\\n```\\n\\nAfter running the script, log into the LangSmith UI:\\n- **Runs** → see each `answer_question` execution with prompt/response.\\n- **Datasets → qa_demo** → view evaluation scores.\\n- **Dashboards** → monitor latency, token cost, and accuracy trends.\\n\\n---\\n\\n## Bottom Line\\n\\nLangSmith is the **observability & evaluation backbone** for any production‑grade LangChain (or other LLM) application. By automatically capturing runs, providing deep tracing, and offering a flexible evaluation framework, it turns the “black‑box” nature of LLMs into a measurable, debuggable, and collaborative workflow. Whether you’re a solo developer prototyping a chatbot or an enterprise team maintaining a fleet of agents, LangSmith gives you the data and tools needed to keep performance, cost, and quality under control.' additional_kwargs={'reasoning_content': 'The user asks \"Can you tell me about Langsmith?\" They want information about Langsmith. Likely they refer to LangSmith, a product from LangChain for LLM observability, debugging, evaluation, etc. Provide overview, features, use cases, pricing, integration, alternatives, etc. Should be concise but thorough. No disallowed content. Provide answer.'} response_metadata={'token_usage': {'completion_tokens': 2736, 'prompt_tokens': 96, 'total_tokens': 2832, 'completion_time': 5.863552396, 'completion_tokens_details': {'reasoning_tokens': 75}, 'prompt_time': 0.00354905, 'prompt_tokens_details': None, 'queue_time': 0.05438802, 'total_time': 5.867101446}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_fd1fe7f861', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--21c0ad7a-800b-47b2-bca2-e8cbda6a5833-0' usage_metadata={'input_tokens': 96, 'output_tokens': 2736, 'total_tokens': 2832, 'output_token_details': {'reasoning': 75}}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangSmith – Overview, Core Features, and How to Use It**\n",
      "\n",
      "---\n",
      "\n",
      "## 1. What Is LangSmith?\n",
      "\n",
      "**LangSmith** is a developer‑focused observability and debugging platform built by the creators of **LangChain**. It is designed to give you visibility into every step of a LangChain (or any LLM‑driven) application—from prompt construction to model inference, tool usage, and final output. Think of it as the “Datadog/Prometheus” for LLM pipelines, but with first‑class support for the unique aspects of generative AI (prompt versions, token usage, tool calls, evaluation metrics, etc.).\n",
      "\n",
      "Key goals:\n",
      "\n",
      "| Goal | Why It Matters |\n",
      "|------|----------------|\n",
      "| **Observability** | Capture the full execution trace of each LLM call, including inputs, outputs, token counts, latency, and any tool/function calls. |\n",
      "| **Debugging** | Replay runs, compare prompt versions, and pinpoint why a model behaved unexpectedly. |\n",
      "| **Evaluation** | Run systematic tests (e.g., unit tests, human‑in‑the‑loop, automatic metrics) across versions and see performance trends. |\n",
      "| **Collaboration** | Share runs, dashboards, and evaluation results with teammates, stakeholders, or customers. |\n",
      "| **Production Guardrails** | Set alerts on cost spikes, latency thresholds, or quality regressions before they affect users. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Core Components & Features\n",
      "\n",
      "| Feature | Description | Typical Use Cases |\n",
      "|---------|-------------|-------------------|\n",
      "| **Run Tracing** | Every LangChain `Runnable` (or any custom function you wrap) can be logged automatically. You get a hierarchical view: **Chain → Prompt → Model → Tool → Output**. | Debugging a failing chain, measuring latency per step, understanding token consumption. |\n",
      "| **Prompt & Model Versioning** | Each run records the exact prompt text, temperature, model name, and any overrides. You can diff prompts across runs. | A/B testing prompt rewrites, rolling back to a known‑good version. |\n",
      "| **Tool/Function Call Logging** | When a model calls a tool (e.g., a database query, API request, or custom function), LangSmith logs the request/response payload. | Auditing external calls, ensuring no private data leaks, debugging tool‑selection logic. |\n",
      "| **Cost & Token Analytics** | Automatic aggregation of token usage per model, cost estimates (based on provider pricing), and per‑run breakdowns. | Budget monitoring, cost‑optimization, reporting to finance. |\n",
      "| **Evaluation Suite** | Define **evaluation datasets** (questions + expected answers) and run them against any version of your chain. Supports custom metrics (BLEU, ROUGE, exact match, cosine similarity, human rating, etc.). | Regression testing after prompt changes, continuous integration pipelines, model selection. |\n",
      "| **Dashboards & UI** | Web UI with searchable runs, filterable by tags, time, model, status, etc. Visualizations for latency, cost, success rates. | Ops monitoring, stakeholder demos, quick health checks. |\n",
      "| **Alerts & Webhooks** | Set thresholds (e.g., latency > 2 s, cost > $500/day, evaluation score drop > 5%). Alerts can be sent to Slack, email, PagerDuty, or custom webhook. | Production guardrails, rapid incident response. |\n",
      "| **Collaboration Tools** | Share a run or a collection of runs via a link, add comments, tag teammates, export to CSV/JSON. | Peer reviews, hand‑offs between dev and product, client demos. |\n",
      "| **API & SDK** | REST API + Python SDK (`langsmith`) that integrates with LangChain, LangGraph, or any custom code. | Automate logging from non‑Python services, embed in CI/CD pipelines. |\n",
      "| **Self‑Hosted / Enterprise Options** | For highly regulated environments, LangSmith offers a self‑hosted version (Docker/K8s) with SSO, audit logs, and on‑prem data residency. | Healthcare, finance, government, or any org with strict data policies. |\n",
      "| **Integrations** | - **LangChain** (auto‑instrumented)<br>- **LangGraph** (graph‑level tracing)<br>- **OpenAI, Anthropic, Cohere, Azure, Vertex, etc.** (model metadata)<br>- **Datadog, Sentry, New Relic** (metrics export) | Plug‑and‑play for existing stacks. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. How It Works Under the Hood\n",
      "\n",
      "1. **Instrumentation** – LangSmith provides a thin wrapper (`langsmith.run`, `langsmith.traceable`) that you add around any `Runnable`. When a run starts, a **run ID** is generated and sent to the LangSmith backend.\n",
      "\n",
      "2. **Data Capture** – As the chain executes, each step posts a JSON payload:\n",
      "   ```json\n",
      "   {\n",
      "     \"run_id\": \"...\",\n",
      "     \"parent_id\": \"...\",\n",
      "     \"type\": \"prompt|model|tool|chain\",\n",
      "     \"input\": \"...\",\n",
      "     \"output\": \"...\",\n",
      "     \"metadata\": {\"model\": \"gpt-4o\", \"temperature\": 0.2, \"tokens\": {\"prompt\": 45, \"completion\": 120}},\n",
      "     \"timestamp\": \"2025-11-27T14:23:10Z\"\n",
      "   }\n",
      "   ```\n",
      "\n",
      "3. **Storage** – The backend stores these events in a time‑series + document store, enabling fast filtering and hierarchical reconstruction.\n",
      "\n",
      "4. **Processing** – Optional background workers compute:\n",
      "   * Token cost estimates (using provider pricing tables)\n",
      "   * Evaluation metric scores (if an evaluation dataset is attached)\n",
      "   * Aggregated statistics for dashboards\n",
      "\n",
      "5. **Visualization** – The UI queries the backend for runs, aggregates, and renders interactive graphs.\n",
      "\n",
      "> **Note:** All data is transmitted over TLS, and you can enable **encryption‑at‑rest** (AES‑256) for the self‑hosted version. Sensitive payloads (e.g., PII) can be redacted via a custom `mask` function you provide.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Getting Started (Python Example)\n",
      "\n",
      "### 4.1 Install the SDK\n",
      "\n",
      "```bash\n",
      "pip install langsmith langchain\n",
      "```\n",
      "\n",
      "### 4.2 Set Your API Key\n",
      "\n",
      "```python\n",
      "import os\n",
      "os.environ[\"LANGSMITH_API_KEY\"] = \"ls_XXXXXXXXXXXXXXXX\"\n",
      "```\n",
      "\n",
      "> You can also store the key in a `.env` file or a secret manager.\n",
      "\n",
      "### 4.3 Instrument a Simple LangChain\n",
      "\n",
      "```python\n",
      "from langchain import OpenAI, LLMChain, PromptTemplate\n",
      "from langsmith import traceable\n",
      "\n",
      "# Wrap the LLM with LangSmith tracing\n",
      "@traceable\n",
      "def traced_openai(**kwargs):\n",
      "    return OpenAI(**kwargs)\n",
      "\n",
      "# Prompt\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"question\"],\n",
      "    template=\"You are a helpful assistant. Answer the following question concisely:\\n{question}\"\n",
      ")\n",
      "\n",
      "# Chain\n",
      "chain = LLMChain(\n",
      "    llm=traced_openai(model=\"gpt-4o-mini\", temperature=0.0),\n",
      "    prompt=prompt\n",
      ")\n",
      "\n",
      "# Run it\n",
      "response = chain.run({\"question\": \"What is the capital of France?\"})\n",
      "print(response)\n",
      "```\n",
      "\n",
      "- The `@traceable` decorator automatically logs the LLM call, its prompt, token usage, and output.\n",
      "- In the LangSmith UI you’ll see a **run tree**:\n",
      "  ```\n",
      "  Run 12345\n",
      "  └─ Prompt → Model (gpt-4o-mini) → Output\n",
      "  ```\n",
      "\n",
      "### 4.4 Adding Evaluation\n",
      "\n",
      "```python\n",
      "from langsmith import evaluate, Dataset\n",
      "\n",
      "# Create a tiny evaluation dataset\n",
      "dataset = Dataset.from_records([\n",
      "    {\"inputs\": {\"question\": \"Who wrote '1984'?\"}, \"outputs\": {\"answer\": \"George Orwell\"}},\n",
      "    {\"inputs\": {\"question\": \"What is 2+2?\"}, \"outputs\": {\"answer\": \"4\"}}\n",
      "])\n",
      "\n",
      "def exact_match(pred, target):\n",
      "    return pred.strip().lower() == target.strip().lower()\n",
      "\n",
      "# Run evaluation\n",
      "results = evaluate(\n",
      "    chain,\n",
      "    dataset=dataset,\n",
      "    evaluator=exact_match,\n",
      "    prediction_key=\"answer\"\n",
      ")\n",
      "\n",
      "print(results.summary())\n",
      "```\n",
      "\n",
      "- The UI now shows a **scorecard** with pass/fail counts, per‑prompt version performance, and a trend line as you iterate.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Typical Workflows\n",
      "\n",
      "| Stage | LangSmith Role |\n",
      "|-------|----------------|\n",
      "| **Prototype** | Quick UI inspection of runs, identify high‑latency steps, compare prompt drafts. |\n",
      "| **CI/CD** | `langsmith evaluate` as a test step; fail the pipeline if regression > X %. |\n",
      "| **Production Monitoring** | Dashboards + alerts for cost spikes, latency outliers, or drop in evaluation score. |\n",
      "| **Post‑mortem** | Replay a failing run, see exact tool payloads, and share the trace with the team. |\n",
      "| **Compliance** | Export audit logs, enforce data redaction, and store runs in a compliant bucket (e.g., AWS GovCloud). |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Pricing (as of 2025)\n",
      "\n",
      "| Tier | Monthly Price | Included Runs | Retention | Key Extras |\n",
      "|------|----------------|---------------|-----------|------------|\n",
      "| **Free** | $0 | 5 k runs / month | 7 days | Basic UI, community support |\n",
      "| **Pro** | $49 | 100 k runs | 30 days | Alerts, evaluation, Slack integration |\n",
      "| **Team** | $199 | 500 k runs | 90 days | Role‑based access, SSO, advanced dashboards |\n",
      "| **Enterprise** | Custom | Unlimited | 1 yr+ (configurable) | Self‑hosted, on‑prem data residency, dedicated support, custom SLAs |\n",
      "\n",
      "> **Run** = a single top‑level trace (including all child steps). Runs are cheap to generate; the cost driver is storage/retention and advanced features.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Security & Compliance Highlights\n",
      "\n",
      "| Aspect | Details |\n",
      "|--------|---------|\n",
      "| **Transport** | TLS 1.3 enforced for all API calls |\n",
      "| **At‑Rest Encryption** | AES‑256 (managed by LangSmith SaaS) or user‑managed keys for self‑hosted |\n",
      "| **PII Redaction** | Provide a `mask` function that runs on every payload before it is sent |\n",
      "| **Audit Logs** | Immutable log of who created/modified runs, with timestamps |\n",
      "| **Compliance** | SOC 2 Type II, ISO 27001, GDPR‑ready; self‑hosted can be placed in EU/US‑Gov regions |\n",
      "| **Access Controls** | Role‑based (Viewer, Analyst, Admin) + optional SSO (Okta, Azure AD, Google Workspace) |\n",
      "\n",
      "---\n",
      "\n",
      "## 8. When (and When Not) to Use LangSmith\n",
      "\n",
      "### ✅ Good Fit\n",
      "\n",
      "- **LLM‑centric products** (chatbots, agents, document‑QA, code generation) that need reliability.\n",
      "- Teams **iterating rapidly** on prompts and want instant feedback.\n",
      "- Organizations that must **track cost** and stay within a budget.\n",
      "- Projects requiring **regulatory audit trails** (healthcare, finance, legal).\n",
      "- Anyone using **LangChain/LangGraph** already—instrumentation is almost zero‑config.\n",
      "\n",
      "### ❌ Not Ideal\n",
      "\n",
      "- **One‑off scripts** that run a handful of times; the free tier may be overkill.\n",
      "- Pure **non‑LLM micro‑services** where traditional observability tools already cover the needs.\n",
      "- Environments where **no external network traffic** is allowed (unless you run the self‑hosted version behind a firewall).\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Alternatives & Complementary Tools\n",
      "\n",
      "| Tool | Focus | How It Relates |\n",
      "|------|-------|----------------|\n",
      "| **PromptLayer** | Prompt versioning & logging (OpenAI‑centric) | Overlaps on prompt tracking; LangSmith is broader (tool calls, evaluation, cost). |\n",
      "| **Arize AI** | Model monitoring & drift detection (mostly embeddings) | Great for embedding models; can be combined with LangSmith for end‑to‑end traces. |\n",
      "| **Weights & Biases (W&B) – LLM Tracking** | Experiment tracking, dataset versioning | Can log LangSmith runs as artifacts; some teams use both for richer analytics. |\n",
      "| **OpenTelemetry** | General distributed tracing | LangSmith implements its own trace format but can export to OTLP for unified observability stacks. |\n",
      "| **LangChain Community Tracing** | Minimal tracing via `ConsoleCallbackHandler` | Useful for debugging locally; LangSmith is the production‑grade upgrade. |\n",
      "\n",
      "---\n",
      "\n",
      "## 10. Quick Checklist for a First‑Time Integration\n",
      "\n",
      "1. **Create a LangSmith account** → get API key.  \n",
      "2. **Install SDK** (`pip install langsmith`).  \n",
      "3. **Wrap your LLM calls** (`@traceable` or `langsmith.run`).  \n",
      "4. **Run a few queries** → verify they appear in the UI.  \n",
      "5. **Define an evaluation dataset** → run `langsmith evaluate`.  \n",
      "6. **Set up alerts** (e.g., Slack webhook for latency > 2 s).  \n",
      "7. **Add role‑based users** and enable SSO if needed.  \n",
      "8. (Optional) **Deploy self‑hosted** if you need on‑prem data residency.\n",
      "\n",
      "---\n",
      "\n",
      "## 11. Resources\n",
      "\n",
      "| Resource | Link |\n",
      "|----------|------|\n",
      "| **LangSmith Documentation** | https://docs.langchain.com/docs/langsmith |\n",
      "| **Python SDK Reference** | https://python.langchain.com/en/latest/reference/langsmith.html |\n",
      "| **Quickstart Notebook** | https://github.com/langchain-ai/langsmith-quickstart |\n",
      "| **Pricing Page** | https://www.langchain.com/langsmith/pricing |\n",
      "| **Self‑Hosted Docker Compose** | https://github.com/langchain-ai/langsmith-selfhost |\n",
      "| **Community Forum** | https://community.langchain.com/c/langsmith |\n",
      "\n",
      "---\n",
      "\n",
      "### TL;DR\n",
      "\n",
      "LangSmith is a purpose‑built observability platform for LLM‑driven applications. It gives you **run‑level tracing, prompt/version tracking, cost analytics, systematic evaluation, and production alerts**—all tightly integrated with LangChain/LangGraph but usable with any Python (or REST) workflow. It’s especially valuable when you need to **debug complex chains, keep AI costs under control, and demonstrate compliance** for production AI systems.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentnotes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
